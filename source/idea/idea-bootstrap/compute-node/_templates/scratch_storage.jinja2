# Begin: Scratch Storage
{% if context.vars.job.params.fsx_lustre.enabled %}
{% include '_templates/linux/fsx_lustre_client.jinja2' %}
function setup_scratch_storage_fsx_for_lustre () {
  FSX_MOUNTPOINT="{{ context.config.get_string('scheduler.scratch_storage.fsx_lustre.mount_point', '/fsx') }}"
  AWS_DNS_SUFFIX="{{ context.config.get_string('cluster.aws.dns_suffix', required=True) }}"
  mkdir -p ${FSX_MOUNTPOINT}
  chmod 777 ${FSX_MOUNTPOINT}
  {% if context.vars.job.params.fsx_lustre.existing_fsx %}
  add_fsx_lustre_to_fstab "{{ context.vars.job.params.fsx_lustre.existing_fsx }}" "${FSX_MOUNTPOINT}"
  {% else %}
  # wait for new scratch FSx to be ready and then add to fstab
  local COMPUTE_STACK="{{ context.vars.job.get_compute_stack() }}"
  local AWS_REGION="{{ context.aws_region }}"
  local AWS=$(command -v aws)
  local FSX_ARN=$($AWS resourcegroupstaggingapi get-resources \
                  --tag-filters "Key=res:FSx,Values=true" "Key=res:StackId,Values=${COMPUTE_STACK}" \
                  --query ResourceTagMappingList[].ResourceARN \
                  --region "${AWS_REGION}" \
                  --output text)
  local FSX_ID=$(echo ${FSX_ARN} | cut -d/ -f2)

  local FSX_DNS="${FSX_ID}.fsx.${AWS_REGION}.${AWS_DNS_SUFFIX}"

  local CHECK_FSX_STATUS=$($AWS fsx describe-file-systems \
                                --file-system-ids "${FSX_ID}" \
                                --query FileSystems[].Lifecycle \
                                --region "${AWS_REGION}" \
                                --output text)

  local LOOP_COUNT=1
  while [[ "${CHECK_FSX_STATUS}" != "AVAILABLE" ]] && [[ ${LOOP_COUNT} -lt 10 ]]
  do
    echo "FSX does not seem to be in AVAILABLE status yet ... waiting 60 secs"
    sleep 60
    CHECK_FSX_STATUS=$($AWS fsx describe-file-systems \
                                      --file-system-ids "${FSX_ID}" \
                                      --query FileSystems[].Lifecycle \
                                      --region "${AWS_REGION}" \
                                      --output text)
    ((LOOP_COUNT++))
  done

  if [[ "${CHECK_FSX_STATUS}" == "AVAILABLE" ]]; then
    echo "FSx is AVAILABLE"
    add_fsx_lustre_to_fstab "${FSX_DNS}" "${FSX_MOUNTPOINT}"
  else
    echo "FSx is not available even after 10 minutes timeout, ignoring FSx mount."
  fi
  {% endif %}
}
setup_scratch_storage_fsx_for_lustre
{% elif context.vars.job.params.scratch_storage_size.value > 0 %}
function setup_scratch_storage_ebs () {
  local SCRATCH_SIZE={{ context.vars.job.params.scratch_storage_size.int_val() }}
  local SCRATCH_MOUNTPOINT="{{ context.config.get_string('scheduler.scratch_storage.ebs.mount_point', '/scratch') }}"

  local LIST_ALL_DISKS=$(lsblk --list | grep disk | awk '{print $1}')
  for disk in ${LIST_ALL_DISKS};
  do
      local CHECK_IF_PARTITION_EXIST=$(lsblk -b /dev/${disk} | grep part | wc -l)
      local CHECK_PARTITION_SIZE=$(lsblk -lnb /dev/${disk} -o SIZE)
      let SCRATCH_SIZE_IN_BYTES=${SCRATCH_SIZE}*1024*1024*1024
      if [[ ${CHECK_IF_PARTITION_EXIST} -eq 0 ]] && [[ ${CHECK_PARTITION_SIZE} -eq ${SCRATCH_SIZE_IN_BYTES} ]]; then
          echo "Detected /dev/${disk} with no partition as scratch device"
          mkfs -t ext4 /dev/${disk}
          mkdir -p ${SCRATCH_MOUNTPOINT}
          chmod 777 ${SCRATCH_MOUNTPOINT}
          echo "/dev/${disk} ${SCRATCH_MOUNTPOINT} ext4 defaults 0 0" >> /etc/fstab
      fi
  done
}
setup_scratch_storage_ebs
{% else %}
function setup_scratch_storage_instance_store () {
  local SCRATCH_MOUNTPOINT="{{ context.config.get_string('scheduler.scratch_storage.instance_store.mount_point', '/scratch') }}"
  {% raw %}
  # Use Instance Store if possible.
  local DEVICES=()
  if [[ ! -z $(ls /dev/nvme[0-9]n1) ]]; then
      echo 'Detected Instance Store: NVME'
      DEVICES=$(ls /dev/nvme[0-9]n1)
  elif [[ ! -z $(ls /dev/xvdc[a-z]) ]]; then
      echo 'Detected Instance Store: SSD'
      DEVICES=$(ls /dev/xvdc[a-z])
  fi

  if [[ -z ${DEVICES} ]]; then
      echo 'No instance store detected on this machine.'
      return 0
  fi

  echo "Detected Instance Store with NVME: ${DEVICES}"

  # Clear Devices which are already mounted (eg: when customer import their own AMI)
  local VOLUME_LIST=()
  for device in ${DEVICES};
  do
    local CHECK_IF_PARTITION_EXIST=$(lsblk -b ${device} | grep part | wc -l)
    if [[ ${CHECK_IF_PARTITION_EXIST} -eq 0 ]]; then
        echo "${device} is free and can be used"
        VOLUME_LIST+=(${device})
    fi
  done

  local VOLUME_COUNT=${#VOLUME_LIST[@]}
  if [[ ${VOLUME_COUNT} -eq 0 ]]; then
    echo "All volumes detected already have a partition or mount point and can't be used as scratch devices"
    return 0
  fi

  if [[ ${VOLUME_COUNT} -eq 1 ]]; then

      # If only 1 instance store, mfks as ext4
      echo "Detected 1 NVMe device available, formatting as ext4 .."
      mkfs -t ext4 ${VOLUME_LIST}
      mkdir -p ${SCRATCH_MOUNTPOINT}
      chmod 777 ${SCRATCH_MOUNTPOINT}
      echo "${VOLUME_LIST} ${SCRATCH_MOUNTPOINT} ext4 defaults,nofail 0 0" >> /etc/fstab

  elif [[ ${VOLUME_COUNT} -gt 1 ]]; then

    # When instance has more than 1 instance store, raid + mount them as /scratch
    echo "Detected more than 1 NVMe device available, creating XFS fs ..."
    local DEVICE_NAME="md0"
    for dev in ${VOLUME_LIST[@]};
    do
      dd if=/dev/zero of=${dev} bs=1M count=1
    done
    echo yes | mdadm --create \
                     --force \
                     --verbose \
                     --level=0 \
                     --raid-devices=${VOLUME_COUNT} /dev/${DEVICE_NAME} ${VOLUME_LIST[@]}
    mkfs -t ext4 /dev/${DEVICE_NAME}
    mdadm --detail --scan | tee -a /etc/mdadm.conf
    mkdir -p ${SCRATCH_MOUNTPOINT}
    chmod 777 ${SCRATCH_MOUNTPOINT}
    echo "/dev/${DEVICE_NAME} ${SCRATCH_MOUNTPOINT} ext4 defaults,nofail 0 0" >> /etc/fstab
  fi
  {% endraw %}
}
setup_scratch_storage_instance_store
{% endif %}
# End: Scratch Storage


